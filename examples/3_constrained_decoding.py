"""
æ–¹æ¡ˆä¸‰ï¼šçº¦æŸè§£ç 
åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶çº¦æŸtokené€‰æ‹©ï¼Œä»æ ¹æœ¬ä¸Šä¿è¯è¾“å‡ºæ ¼å¼æ­£ç¡®æ€§
"""

import os
import json
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from typing import Dict, Any, Set, Optional, List
from enum import Enum
from dataclasses import dataclass

# æ•°æ®æ¨¡å‹
class ProductInfo(BaseModel):
    name: str = Field(description="äº§å“åç§°")
    price: float = Field(ge=0, description="äº§å“ä»·æ ¼")
    currency: str = Field(default="USD", description="è´§å¸å•ä½")
    category: str = Field(description="äº§å“ç±»åˆ«")
    features: List[str] = Field(description="äº§å“ç‰¹æ€§")
    in_stock: bool = Field(description="æ˜¯å¦æœ‰åº“å­˜")

@dataclass
class GrammarState:
    """è¯­æ³•çŠ¶æ€è·Ÿè¸ª"""
    position: str  # å½“å‰ä½ç½®: start, object_start, key_start, key_content, key_end, value_start, etc.
    expected_tokens: Set[str]  # æœŸæœ›çš„tokené›†åˆ
    depth: int  # åµŒå¥—æ·±åº¦
    in_string: bool  # æ˜¯å¦åœ¨å­—ç¬¦ä¸²ä¸­
    string_delimiter: Optional[str] = None
    current_key: Optional[str] = None  # å½“å‰å¤„ç†çš„é”®
    brace_count: int = 0  # å¤§æ‹¬å·è®¡æ•°

class JSONGrammarValidator:
    """JSONè¯­æ³•éªŒè¯å™¨ - çº¦æŸè§£ç çš„æ ¸å¿ƒç»„ä»¶"""

    def __init__(self, schema: Dict[str, Any] = None):
        self.schema = schema or {}
        self.required_keys = set(schema.get("required", []))
        self.allowed_keys = set(schema.get("properties", {}).keys())

    def get_next_allowed_tokens(self, state: GrammarState, text_so_far: str) -> Set[str]:
        """æ ¹æ®å½“å‰è¯­æ³•çŠ¶æ€è·å–å…è®¸çš„tokené›†åˆ"""
        allowed = set()

        # åŸºç¡€å­—ç¬¦é›†åˆ
        basic_chars = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
        basic_chars.update(set('., -_'))
        allowed.update(basic_chars)

        # æ ¹æ®çŠ¶æ€çº¦æŸ
        if state.position == "start":
            return {"{"}

        elif state.position == "object_start":
            allowed.update({'"', "}"})

        elif state.position == "key_start":
            return {'"'}

        elif state.position == "key_content":
            allowed.update(set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_'))

        elif state.position == "key_end":
            return {':'}

        elif state.position == "value_start":
            # æ ¹æ®é”®åå’Œschemaç¡®å®šå€¼ç±»å‹
            if state.current_key == "name" or state.current_key == "category":
                return {'"'}
            elif state.current_key == "price":
                return set('0123456789')
            elif state.current_key == "in_stock":
                return set('tfn')  # true, false, null
            elif state.current_key == "features":
                return {'['}
            elif state.current_key == "currency":
                return {'"'}
            else:
                return {'"', '0', 't', 'f', 'n', '['}  # string, number, true, false, null, array

        elif state.position == "string_content":
            if state.string_delimiter == '"':
                allowed.update(set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .-_/'))
            allowed.update({'"',})

        elif state.position == "array_content":
            allowed.update(set('"0123456789tfn'))
            allowed.update({']', ','})

        elif state.position == "object_key_separator":
            allowed.update({',', '}'})

        return allowed

    def update_state(self, state: GrammarState, token: str) -> GrammarState:
        """æ ¹æ®æ–°tokenæ›´æ–°è¯­æ³•çŠ¶æ€"""
        new_state = GrammarState(
            position=state.position,
            expected_tokens=state.expected_tokens.copy(),
            depth=state.depth,
            in_string=state.in_string,
            string_delimiter=state.string_delimiter,
            current_key=state.current_key,
            brace_count=state.brace_count
        )

        # çŠ¶æ€è½¬æ¢é€»è¾‘
        if token == '{':
            new_state.position = "object_start"
            new_state.depth += 1
            new_state.brace_count += 1
            new_state.in_string = False

        elif token == '}':
            new_state.position = "object_end"
            new_state.depth -= 1
            new_state.brace_count -= 1

        elif token == '"':
            if not new_state.in_string:
                if new_state.position in ["object_start", "object_key_separator"]:
                    new_state.position = "key_start"
                elif new_state.position in ["value_start"]:
                    new_state.position = "string_content"
                new_state.in_string = True
                new_state.string_delimiter = '"'
            else:
                new_state.in_string = False
                new_state.string_delimiter = None
                if new_state.position == "key_content":
                    new_state.position = "key_end"
                elif new_state.position == "string_content":
                    new_state.position = "object_key_separator"

        elif token == ':':
            if new_state.position == "key_end":
                new_state.position = "value_start"

        elif token == ',':
            if new_state.position == "object_key_separator":
                new_state.position = "key_start"
            elif new_state.position == "array_content":
                new_state.position = "array_content"

        elif token == '[':
            if new_state.position == "value_start" and new_state.current_key == "features":
                new_state.position = "array_content"

        elif token == ']':
            if new_state.position == "array_content":
                new_state.position = "object_key_separator"

        elif token.isalnum():
            if new_state.position == "key_start":
                new_state.position = "key_content"
                new_state.current_key = token
            elif new_state.position == "key_content":
                new_state.current_key += token
            elif new_state.position == "string_content":
                # å­—ç¬¦ä¸²å†…å®¹ï¼Œä¿æŒçŠ¶æ€
                pass
            elif new_state.position == "value_start" and token in "tfn":
                # true, false, nullçš„å¼€å§‹
                pass

        return new_state

    def validate_partial_json(self, text: str) -> bool:
        """éªŒè¯éƒ¨åˆ†JSONæ˜¯å¦ç¬¦åˆè¯­æ³•"""
        try:
            state = GrammarState(
                position="start",
                expected_tokens=set(),
                depth=0,
                in_string=False,
                brace_count=0
            )

            for char in text:
                # è·å–å½“å‰å…è®¸çš„token
                allowed_tokens = self.get_next_allowed_tokens(state, text)

                # æ£€æŸ¥å­—ç¬¦æ˜¯å¦è¢«å…è®¸
                if char not in allowed_tokens:
                    return False

                # æ›´æ–°çŠ¶æ€
                state = self.update_state(state, char)

            # æ£€æŸ¥æœ€ç»ˆçŠ¶æ€
            return state.brace_count == 0 and (state.position == "object_end" or state.position == "object_key_separator")

        except Exception:
            return False

class ConstrainedTokenGenerator:
    """çº¦æŸTokenç”Ÿæˆå™¨"""

    def __init__(self, llm):
        self.llm = llm
        self.grammar_validator = JSONGrammarValidator()

    def generate_with_constraints(self, prompt: str, max_length: int = 500) -> str:
        """ä½¿ç”¨çº¦æŸç”Ÿæˆæ–‡æœ¬"""
        try:
            generated_text = ""
            state = GrammarState(
                position="start",
                expected_tokens=set(),
                depth=0,
                in_string=False,
                brace_count=0
            )

            # ä½¿ç”¨ç®€å•çš„çº¦æŸç”Ÿæˆç­–ç•¥
            for step in range(20):  # é™åˆ¶æ­¥æ•°é¿å…æ— é™å¾ªç¯
                # è·å–å½“å‰å…è®¸çš„token
                allowed_tokens = self.grammar_validator.get_next_allowed_tokens(state, generated_text)

                # æ„å»ºçº¦æŸæç¤º
                constraint_prompt = f"""
{prompt}

å½“å‰ç”Ÿæˆçš„JSONï¼š{generated_text}

è¯·ç»§ç»­ç”Ÿæˆï¼Œåªä½¿ç”¨ä»¥ä¸‹å­—ç¬¦ï¼š{sorted(list(allowed_tokens))}
æ¯ä¸ªå­—ç¬¦éƒ½å¿…é¡»ç¬¦åˆJSONè¯­æ³•è§„åˆ™ã€‚
è¯·åªè¾“å‡ºæ¥ä¸‹æ¥çš„å­—ç¬¦ï¼Œä¸è¦è§£é‡Šã€‚
"""

                # è·å–ä¸‹ä¸€ä¸ªå­—ç¬¦
                response = self.llm.invoke(constraint_prompt)
                next_char = response.content.strip()[:1]  # åªå–ç¬¬ä¸€ä¸ªå­—ç¬¦

                # éªŒè¯å­—ç¬¦
                if next_char in allowed_tokens:
                    generated_text += next_char
                    state = self.grammar_validator.update_state(state, next_char)

                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if state.brace_count == 0 and state.position != "start":
                        break
                else:
                    # å­—ç¬¦ä¸è¢«å…è®¸ï¼Œå°è¯•æ‰¾åˆ°æœ€æ¥è¿‘çš„æœ‰æ•ˆå­—ç¬¦
                    closest_char = self._find_closest_valid_char(next_char, allowed_tokens)
                    if closest_char:
                        generated_text += closest_char
                        state = self.grammar_validator.update_state(state, closest_char)

            return generated_text

        except Exception as e:
            print(f"çº¦æŸç”Ÿæˆå¤±è´¥: {str(e)}")
            return ""

    def _find_closest_valid_char(self, char: str, allowed_tokens: Set[str]) -> Optional[str]:
        """æ‰¾åˆ°æœ€æ¥è¿‘çš„æœ‰æ•ˆå­—ç¬¦"""
        # ç®€å•çš„ç­–ç•¥ï¼šæŸ¥æ‰¾åŒ…å«åœ¨å…è®¸å­—ç¬¦ä¸­çš„å­—ç¬¦
        for c in char:
            if c in allowed_tokens:
                return c

        # é»˜è®¤å°è¯•
        if '"' in allowed_tokens and state.position in ["object_start", "key_start", "value_start"]:
            return '"'
        elif '}' in allowed_tokens:
            return '}'
        elif ':' in allowed_tokens:
            return ':'
        elif ',' in allowed_tokens:
            return ','

        return None

class ConstrainedDecodingSystem:
    """çº¦æŸè§£ç ç³»ç»Ÿ"""

    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.getenv("SILICONFLOW_API_KEY"),
            model="deepseek-ai/DeepSeek-V3.1",
            base_url="https://api.siliconflow.cn/v1/",
            temperature=0.1
        )
        self.generator = ConstrainedTokenGenerator(self.llm)

    def generate_structured_output(self, prompt: str, schema: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆç»“æ„åŒ–è¾“å‡º"""
        try:
            # è®¾ç½®è¯­æ³•éªŒè¯å™¨
            self.generator.grammar_validator = JSONGrammarValidator(schema)

            # æ„å»ºç»“æ„åŒ–æç¤º
            structured_prompt = f"""
è¯·ç”Ÿæˆäº§å“ä¿¡æ¯çš„JSONæ ¼å¼è¾“å‡ºã€‚

è¦æ±‚æ ¼å¼ï¼š
{json.dumps(schema, indent=2, ensure_ascii=False)}

ç”¨æˆ·éœ€æ±‚ï¼š{prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ‰€æœ‰æ‹¬å·ã€å¼•å·éƒ½æ­£ç¡®åŒ¹é…ã€‚
"""

            # çº¦æŸç”Ÿæˆ
            generated_json = self.generator.generate_with_constraints(structured_prompt)

            # éªŒè¯è¾“å‡º
            try:
                parsed_data = json.loads(generated_json)
                return parsed_data
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•åå¤„ç†ä¿®å¤
                return self._post_process_json(generated_json)

        except Exception as e:
            print(f"çº¦æŸè§£ç å¤±è´¥: {str(e)}")
            return None

    def _post_process_json(self, text: str) -> Optional[Dict[str, Any]]:
        """åå¤„ç†ä¿®å¤JSON"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = json.loads(text)
            return json_match
        except:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼ä¿®å¤
            return self._heuristic_json_repair(text)

    def _heuristic_json_repair(self, text: str) -> Optional[Dict[str, Any]]:
        """å¯å‘å¼JSONä¿®å¤"""
        try:
            # ç¡®ä¿ä»¥ { å¼€å¤´ï¼Œä»¥ } ç»“å°¾
            if not text.strip().startswith('{'):
                text = '{' + text.strip()
            if not text.strip().endswith('}'):
                text = text.strip() + '}'

            # åŸºæœ¬éªŒè¯
            json.loads(text)
            return json.loads(text)
        except:
            return None

def main():
    """ä¸»å‡½æ•°æ¼”ç¤ºçº¦æŸè§£ç """
    print("ğŸ”§ çº¦æŸè§£ç æ¼”ç¤º")
    print("="*50)

    # åˆ›å»ºçº¦æŸè§£ç ç³»ç»Ÿ
    system = ConstrainedDecodingSystem()

    # å®šä¹‰JSON Schema
    product_schema = {
        "type": "object",
        "properties": {
            "name": {"type": "string"},
            "price": {"type": "number"},
            "currency": {"type": "string", "default": "USD"},
            "category": {"type": "string"},
            "features": {"type": "array"},
            "in_stock": {"type": "boolean"}
        },
        "required": ["name", "price", "category"]
    }

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "iPhone 15 Pro Maxï¼Œè‹¹æœæœ€æ–°æ——èˆ°ï¼Œé’›åˆé‡‘è®¾è®¡ï¼Œå”®ä»·1199ç¾å…ƒï¼Œæœ‰åº“å­˜",
        "æˆ´æ£®V15å¸å°˜å™¨ï¼Œå”®ä»·4990å…ƒï¼Œè¶…å¼ºå¸åŠ›ï¼Œæ¿€å…‰æ¢æµ‹æŠ€æœ¯",
        "ç‰¹æ–¯æ‹‰Model Yï¼Œç”µåŠ¨æ±½è½¦ï¼Œç»­èˆªé‡Œç¨‹600å…¬é‡Œï¼Œå”®ä»·4ä¸‡ç¾å…ƒï¼Œåº“å­˜ç´§å¼ ",
        "å°ç±³æ‰‹ç¯8ï¼Œæ™ºèƒ½ç©¿æˆ´è®¾å¤‡ï¼Œå”®ä»·299å…ƒï¼Œå¿ƒç‡ç›‘æµ‹ï¼Œé˜²æ°´åŠŸèƒ½"
    ]

    for i, test_input in enumerate(test_cases, 1):
        print(f"\nğŸ§ª æµ‹è¯•ç”¨ä¾‹ {i}: {test_input}")
        print("-" * 40)

        try:
            # ç”Ÿæˆç»“æ„åŒ–è¾“å‡º
            result = system.generate_structured_output(test_input, product_schema)

            if result:
                print("âœ… çº¦æŸè§£ç æˆåŠŸ:")
                print(f"   äº§å“: {result.get('name', 'N/A')}")
                print(f"   ä»·æ ¼: {result.get('price', 'N/A')} {result.get('currency', 'N/A')}")
                print(f"   ç±»åˆ«: {result.get('category', 'N/A')}")
                print(f"   ç‰¹æ€§: {', '.join(result.get('features', []))}")
                print(f"   åº“å­˜: {'æœ‰' if result.get('in_stock') else 'æ— '}")
            else:
                print("âŒ çº¦æŸè§£ç å¤±è´¥")

        except Exception as e:
            print(f"ğŸ’¥ å¤„ç†å¼‚å¸¸: {str(e)}")

        if i < len(test_cases):
            print("\n" + "="*50)

if __name__ == "__main__":
    main()